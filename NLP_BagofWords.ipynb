{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "89f296222c8130915f1e43629237569ba6e5fdb0b27294fe6f64832e34b39e99"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\minma\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hi',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Henrik',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'tech',\n",
       " 'youtuber',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " '21',\n",
       " 'years',\n",
       " 'old']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# 1) Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentence = 'Hi, My name is Henrik, I am a tech youtuber. I am 21 years old'\n",
    "nltk.word_tokenize(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words=stopwords.words('danish')\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "import string\n",
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Filtered Data is: ['hi', 'my', 'name', 'is', 'nachiketa', 'i', 'am', '21', 'years', 'old', 'so', 'think', 'might', 'just', 'open', 'a', 'youtube', 'channel', 'the', 'name', 'will', 'be', 'nachiketa']\nRemoved words: [',', '.', '!', ',', 'i', 'i', '.', ':']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'Hi, My name is Nachiketa. I am 21 years old! So , i think i might just open a youtube channel. The name will be : Nachiketa'\n",
    "filtered_data=[]\n",
    "removed_data=[]\n",
    "for i in nltk.word_tokenize(sentence):\n",
    "    if (i not in stop_words) and (i not in punc):\n",
    "        filtered_data.append(i.lower())\n",
    "    else:\n",
    "        removed_data.append(i)\n",
    "\n",
    "print('Filtered Data is:',filtered_data)\n",
    "print('Removed words:',removed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\minma\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\minma\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Stemming convert down to base word\n",
    "# Lemmatization english word\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Words                         Snowball Stemmer              Lancaster Stemmer             Porter Stemmer                \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "stem() missing 1 required positional argument: 'word'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7bf101678afe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0:30}{1:30}{2:30}{3:30}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Words'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Snowball Stemmer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Lancaster Stemmer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Porter Stemmer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0:30}{1:30}{2:30}{3:30}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSnowball\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlancaster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: stem() missing 1 required positional argument: 'word'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, SnowballStemmer\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer\n",
    "Snowball = SnowballStemmer(\"english\") # also danish\n",
    "\n",
    "words = ['hobbies','computer','running','SUPERB','Computation','friends','cooling','jumping','shipping','riding','gangster']\n",
    "print(\"{0:30}{1:30}{2:30}{3:30}\".format('Words','Snowball Stemmer','Lancaster Stemmer', 'Porter Stemmer')) # .format i Python 3.7?\n",
    "for word in words:\n",
    "    print(\"{0:30}{1:30}{2:30}{3:30}\".format(word,Snowball.stem(word), lancaster.stem(word),porter.stem(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i was go to my school , but i stop in between becaus i want på play some footbal\ni was going to my school , but i stop in between becaus i want på play som footbal\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "stem() missing 1 required positional argument: 'word'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0a32df740981>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSnowball\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlancaster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mporter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mstemm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-0a32df740981>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSnowball\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlancaster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mporter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mstemm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: stem() missing 1 required positional argument: 'word'"
     ]
    }
   ],
   "source": [
    "sentence = \"I was going to my school, but i stopped in between because I wanted på play some football\"\n",
    "token = nltk.word_tokenize(sentence)\n",
    "for stemmer in (Snowball, lancaster, porter):\n",
    "    stemm = [stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['adding', 'an', 'bag', 'example', 'how', 'is', 'just', 'of', 'out', 'see', 'senctece', 'things', 'this', 'to', 'turn', 'words']\n[[1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 3) Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "string = [\"This is an example of bag of words!. Just adding this senctece to see how things turn out\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X=vectorizer.fit_transform(string)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['awful', 'burger', 'delicious', 'food', 'is', 'tasty', 'the']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     awful    burger  delicious      food        is     tasty       the\n",
       "0  0.00000  0.608845    0.00000  0.000000  0.359594  0.608845  0.359594\n",
       "1  0.00000  0.000000    0.66284  0.504107  0.391484  0.000000  0.391484\n",
       "2  0.66284  0.000000    0.00000  0.504107  0.391484  0.000000  0.391484"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>awful</th>\n      <th>burger</th>\n      <th>delicious</th>\n      <th>food</th>\n      <th>is</th>\n      <th>tasty</th>\n      <th>the</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00000</td>\n      <td>0.608845</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.359594</td>\n      <td>0.608845</td>\n      <td>0.359594</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.66284</td>\n      <td>0.504107</td>\n      <td>0.391484</td>\n      <td>0.000000</td>\n      <td>0.391484</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.66284</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.504107</td>\n      <td>0.391484</td>\n      <td>0.000000</td>\n      <td>0.391484</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# 4) Term Frequency - Inverse Document Frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfid = TfidfVectorizer(stop_words={''})\n",
    "\n",
    "doc= [\"The burger is tasty\",\"The food is delicious\", \"The food is awful\"]\n",
    "\n",
    "doc_vector = tfid.fit_transform(doc)\n",
    "print(tfid.get_feature_names())\n",
    "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "df\n",
    "#print(doc_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Sentiment Analysis\n",
    "\n",
    "# SKAL LAVES\n"
   ]
  }
 ]
}